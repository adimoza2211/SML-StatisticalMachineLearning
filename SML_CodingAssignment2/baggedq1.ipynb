{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the mnist dataset once again\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "np.set_printoptions(threshold=np.inf)\n",
    "data = np.load('mnist.npz')\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "#we normalize the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5923, 784)\n",
      "(6742, 784)\n",
      "(5958, 784)\n",
      "(18623,)\n",
      "(18623, 784)\n"
     ]
    }
   ],
   "source": [
    "#selecting class 0,1,2\n",
    "class0 = x_train[y_train == 0]\n",
    "class1 = x_train[y_train == 1]\n",
    "class2 = x_train[y_train == 2]\n",
    "print(class0.shape)\n",
    "print(class1.shape)\n",
    "print(class2.shape)\n",
    "Ylabels = [0]*class0.shape[0] + [1]*class1.shape[0] + [2]*class2.shape[0]\n",
    "Ylabels = np.array(Ylabels)\n",
    "print(Ylabels.shape)\n",
    "concat_classes = np.concatenate((class0,class1,class2), axis= 0)\n",
    "\n",
    "print(concat_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GiniIndex(leftlables, rightlables):\n",
    "    left0count = np.sum(leftlables == 0)\n",
    "    left1count = np.sum(leftlables == 1)\n",
    "    left2count = np.sum(leftlables == 2)\n",
    "    right0count = np.sum(rightlables == 0)\n",
    "    right1count = np.sum(rightlables == 1)\n",
    "    right2count = np.sum(rightlables == 2)\n",
    "    lefttotal = left0count + left1count + left2count\n",
    "    righttotal = right0count + right1count + right2count\n",
    "    giniLeft = 1 - (left0count/lefttotal)**2 - (left1count/lefttotal)**2 - (left2count/lefttotal)**2\n",
    "    giniRight = 1 - (right0count/righttotal)**2 - (right1count/righttotal)**2 - (right2count/righttotal)**2\n",
    "    giniIndex = (lefttotal/(lefttotal + righttotal))*giniLeft + (righttotal/(lefttotal + righttotal))*giniRight\n",
    "    return giniIndex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSplit(Y,mutually_indexed_Ylabels, midpoints_per_dimension, dimension):\n",
    "    leftlables = []\n",
    "    rightlables = []\n",
    "    left = []\n",
    "    right = []\n",
    "    for index, sample in enumerate(Y):\n",
    "        if sample[dimension] < midpoints_per_dimension[dimension]:\n",
    "            left.append(sample)\n",
    "            leftlables.append(mutually_indexed_Ylabels[index])\n",
    "        else:\n",
    "            right.append(sample)\n",
    "            rightlables.append(mutually_indexed_Ylabels[index])\n",
    "    return np.array(left), np.array(right), np.array(leftlables), np.array(rightlables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSplit(Y,Ylabels, midpoints_per_dimension):\n",
    "    gini = np.inf\n",
    "    best_split = None\n",
    "    for i in range(10):\n",
    "        left, right, leftlables, rightlables = makeSplit(Y,Ylabels, midpoints_per_dimension, i)\n",
    "        giniLoop = GiniIndex(leftlables, rightlables)\n",
    "        if giniLoop < gini:\n",
    "            gini = giniLoop\n",
    "            best_split = (i,gini,left,right,leftlables,rightlables)\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample, firstmean, rightsplitmean):\n",
    "    if(sample[0] < firstmean[0]):\n",
    "        return 0\n",
    "    else:\n",
    "        if(sample[0] < rightsplitmean[0]):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCA_Data(data,mean):\n",
    "    data = data - mean\n",
    "    cov = data.T @ data/18622\n",
    "    eigenvalues,eigenvectors = np.linalg.eig(cov)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    U = eigenvectors[:,idx]\n",
    "    U = np.real(U)\n",
    "    U = U[:,:10]\n",
    "    Y = U.T @ data.T\n",
    "    Y = Y.T\n",
    "    Y = np.real(Y/255)\n",
    "    return Y, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnDictWithNumberofClasses(Ylabels):\n",
    "    class_count = {0:np.sum(Ylabels == 0),1:np.sum(Ylabels == 1),2:np.sum(Ylabels == 2)}\n",
    "    return class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBaggedAndReducedData(x_test,x_testLabels):\n",
    "    indices = np.random.choice(18623, 18623, replace = True)\n",
    "    baggedData = x_test[indices]\n",
    "    baggedLabels = x_testLabels[indices]\n",
    "    datamean = np.mean(baggedData, axis = 0)\n",
    "    Y,U = getPCA_Data(baggedData,datamean)\n",
    "    return Y,U,baggedLabels,datamean\n",
    "    #apply the same transformation as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we write the part1 code in a function\n",
    "def trainTree(Y,Ylabels,mean_vector):\n",
    "    midpoints_per_dimension = [mean_vector[i] for i in range(10)]\n",
    "    best_split = bestSplit(Y,Ylabels,midpoints_per_dimension)\n",
    "\n",
    "    newleft = best_split[2]\n",
    "    newleftlabels = best_split[4]\n",
    "    class_count_firstLeft = returnDictWithNumberofClasses(newleftlabels)\n",
    "    newLeftMean = np.mean(newleft, axis = 0)\n",
    "    newLeftMidpoints = [newLeftMean[i] for i in range(10)]\n",
    "\n",
    "    newright = best_split[3]\n",
    "    newrightlabels = best_split[5]\n",
    "    newRightMean = np.mean(newright, axis = 0)\n",
    "    newRightMidpoints = [newRightMean[i] for i in range(10)]\n",
    "\n",
    "    newLeftBestSplit = bestSplit(newleft,newleftlabels,newLeftMidpoints)\n",
    "    newRightBestSplit = bestSplit(newright,newrightlabels,newRightMidpoints)\n",
    "    \n",
    "    newBestSplit = None\n",
    "    if newLeftBestSplit[1] < newRightBestSplit[1]:\n",
    "        newBestSplit = newLeftBestSplit\n",
    "    else:\n",
    "        newBestSplit = newRightBestSplit\n",
    "    class_count_secondLeft = returnDictWithNumberofClasses(newBestSplit[4])\n",
    "    class_count_secondRight = returnDictWithNumberofClasses(newBestSplit[5])\n",
    "    print(best_split[0],best_split[1], newBestSplit[0],newBestSplit[1])\n",
    "    return best_split, newBestSplit, class_count_firstLeft, class_count_secondLeft, class_count_secondRight\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictArbitrary(sample, firstmean, rightsplitmean, class_count_firstLeft, class_count_secondLeft, class_count_secondRight):\n",
    "    if(sample[0] < firstmean[0]):\n",
    "        return max(class_count_firstLeft, key = class_count_firstLeft.get)\n",
    "    else:\n",
    "        if(sample[0] < rightsplitmean[0]):\n",
    "            return max(class_count_secondLeft, key = class_count_secondLeft.get)\n",
    "        else:\n",
    "            return max(class_count_secondRight, key = class_count_secondRight.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTest(test_concat_classes,mean_vector,U):\n",
    "    test_concat_classes_func = test_concat_classes - mean_vector\n",
    "    Y = U.T @ test_concat_classes_func.T\n",
    "    Y = Y.T\n",
    "    Y = np.real(Y/255)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18623, 10)\n",
      "0 0.44158414128431744 0 0.1508673520197507\n",
      "0 0.44158414128431744 0 0.1508673520197507\n",
      "{0: 5844, 1: 10, 2: 2768} {0: 79, 1: 661, 2: 3041} {0: 0, 1: 6071, 2: 149}\n"
     ]
    }
   ],
   "source": [
    "mean_highdim = np.mean(concat_classes, axis = 0)\n",
    "Y,U = getPCA_Data(concat_classes,mean_highdim)\n",
    "print(Y.shape)\n",
    "mean_vector = np.mean(Y, axis = 0)\n",
    "best_split, newBestSplit, class_count_first, class_count_secondLeft, class_count_secondRight = trainTree(Y,Ylabels,mean_vector)\n",
    "print(best_split[0],best_split[1], newBestSplit[0],newBestSplit[1])\n",
    "print(class_count_first, class_count_secondLeft, class_count_secondRight)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 784)\n",
      "(1135, 784)\n",
      "(1032, 784)\n",
      "(3147,)\n"
     ]
    }
   ],
   "source": [
    "test_class0 = x_test[y_test == 0]\n",
    "test_class1 = x_test[y_test == 1]\n",
    "test_class2 = x_test[y_test == 2]\n",
    "print(test_class0.shape)\n",
    "print(test_class1.shape)\n",
    "print(test_class2.shape)\n",
    "testYlabels = [0]*test_class0.shape[0] + [1]*test_class1.shape[0] + [2]*test_class2.shape[0]\n",
    "testYlabels = np.array(testYlabels)\n",
    "print(testYlabels.shape)\n",
    "test_concat_classes = np.concatenate((test_class0,test_class1,test_class2), axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3147, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_test = makeTest(test_concat_classes,mean_highdim,U)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 2 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 2 1 1 2 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1\n",
      " 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 2 2 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 2 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1\n",
      " 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 1 1 2 1 1 2 2 1 2 2 1 2 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2\n",
      " 2 2 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 2 2 2 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n",
      " 2 1 1 1 1 1 2 0 2 2 2 0 2 0 0 0 0 2 0 2 2 0 2 0 0 0 2 2 2 2 2 0 0 2 2 0 2\n",
      " 0 2 2 0 2 2 2 2 2 2 2 0 2 0 2 2 0 2 0 1 2 2 2 0 0 2 2 2 1 0 2 0 2 2 2 2 2\n",
      " 0 0 2 2 2 2 2 2 2 0 2 0 2 2 0 0 2 0 2 2 0 2 2 2 0 2 2 2 0 0 0 2 2 0 2 0 2\n",
      " 0 0 0 2 2 0 2 0 2 0 2 2 0 2 2 0 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 0 2 0\n",
      " 2 2 0 2 2 0 2 2 0 2 0 2 2 2 0 2 0 0 2 2 1 2 2 1 2 2 2 2 0 0 0 2 2 2 2 1 0\n",
      " 0 2 0 2 0 2 2 2 2 0 0 2 2 2 2 2 2 0 2 0 0 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2\n",
      " 2 0 2 0 0 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 0 0 2 1 2 2 2 2 2\n",
      " 2 2 1 2 2 0 2 2 0 2 2 2 2 0 2 2 0 2 2 2 2 0 2 1 0 0 0 2 2 2 0 2 2 2 0 0 2\n",
      " 0 2 0 2 0 0 0 2 0 0 2 2 1 0 2 0 0 0 2 2 2 0 0 2 0 2 0 2 2 0 2 0 2 2 2 2 2\n",
      " 0 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2\n",
      " 2 0 2 2 0 2 2 0 1 2 2 2 2 0 0 1 2 2 0 0 0 2 2 2 2 0 0 0 2 2 2 2 2 0 0 0 2\n",
      " 2 2 2 2 2 2 2 2 0 0 0 2 0 1 0 0 2 2 0 0 0 0 2 2 2 2 2 0 2 0 2 2 2 2 0 2 2\n",
      " 2 2 2 0 2 2 0 2 2 0 2 0 0 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 1 2 2 2 2 2 0 1 0\n",
      " 0 0 0 2 2 2 0 0 2 2 2 0 2 1 2 2 2 2 2 2 0 0 0 2 2 2 2 2 2 1 1 2 2 2 2 0 0\n",
      " 0 2 0 2 0 2 0 0 2 2 2 1 2 2 2 2 2 2 0 2 0 2 2 0 0 0 0 0 0 0 0 0 2 2 0 0 2\n",
      " 2 2 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 2 2 0 0 2 2 2 2 2 0 0 0 0 0\n",
      " 0 0 2 0 0 2 0 0 0 0 2 0 0 2 0 2 0 0 2 2 0 2 0 0 2 0 0 0 2 0 2 2 2 2 2 0 2\n",
      " 2 0 1 2 2 0 2 2 2 2 2 0 2 0 0 0 2 0 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n",
      " 2 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 0 0\n",
      " 0 0 2 0 0 0 2 2 2 2 0 2 0 0 2 2 2 2 1 2 2 2 2 2 2 2 2 2 0 0 0 0 2 0 0 0 0\n",
      " 2 0 2 2 0 0 2 2 0 2 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 2 2\n",
      " 2 2 0 0 2 0 2 2 2 2 2 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2\n",
      " 0 2 2 2 0 2 2 2 2 2 2 0 2 0 0 2 2 0 0 2 2 0 2 0 0 2 2 0 2 0 0 0 0 2 0 2 0\n",
      " 2 2 2 2 2 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 2 2 2 0 0 0 0 2 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 0 2 0 2 2 2 2 2 1 0 0 0\n",
      " 0 0 0 0 0 2 2 2 2 2 2 0 2 0 2 0 0 0 0 2 2 1 0 0 2 0 0 2 0 0 2 0 0 2 2 0 2\n",
      " 0 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0\n",
      " 2 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "rightSplitmean = np.mean(best_split[3], axis = 0)\n",
    "for sample in Y_test:\n",
    "    predictions.append(predictArbitrary(sample,mean_vector,rightSplitmean,class_count_first, class_count_secondLeft, class_count_secondRight))\n",
    "predictions = np.array(predictions)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8090244677470607\n"
     ]
    }
   ],
   "source": [
    "#we calculate the accuracy\n",
    "accuracy = np.sum(predictions == testYlabels)/len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE FUNCTIONS ARE CORRECT. WE NEED TO IMPLEMENT BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18623, 10) (784, 10) (18623,) (784,)\n",
      "(18623, 10) (784, 10) (18623,) (784,)\n",
      "(18623, 10) (784, 10) (18623,) (784,)\n",
      "(18623, 10) (784, 10) (18623,) (784,)\n",
      "(18623, 10) (784, 10) (18623,) (784,)\n"
     ]
    }
   ],
   "source": [
    "#we now implement the bagging for 5 trees\n",
    "Dis = []\n",
    "Di_U = []\n",
    "Di_labels = []\n",
    "Di_mean = []\n",
    "for i in range(5):\n",
    "    Y,U,baggedLabels,datamean = getBaggedAndReducedData(concat_classes,Ylabels)\n",
    "    Di_U.append(U)\n",
    "    Di_labels.append(baggedLabels)\n",
    "    Di_mean.append(datamean)\n",
    "    Dis.append(Y)\n",
    "    print(Y.shape, U.shape, baggedLabels.shape, datamean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4442169460136429 0 0.1530036275871741\n",
      "0 0.4430221062394115 0 0.14882637350713349\n",
      "0 0.4458166656309114 0 0.1597580383695303\n",
      "0 0.4412952767188004 0 0.1583738161130442\n",
      "0 0.4396144307106541 0 0.15039190857364887\n"
     ]
    }
   ],
   "source": [
    "#now we train the trees\n",
    "trees = []\n",
    "for i in range(5):\n",
    "    best_split, newBestSplit, class_count_first, class_count_secondLeft, class_count_secondRight = trainTree(Dis[i],Di_labels[i],Di_mean[i])\n",
    "    trees.append((best_split, newBestSplit, class_count_first, class_count_secondLeft, class_count_secondRight)) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3147, 10)\n",
      "(3147, 10)\n",
      "(3147, 10)\n",
      "(3147, 10)\n",
      "(3147, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_test_arr = []\n",
    "for i in range(5):\n",
    "    Y_test_temp = makeTest(test_concat_classes,Di_mean[i],Di_U[i])\n",
    "    Y_test_arr.append(Y_test_temp)\n",
    "    print(Y_test_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3147,)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(3147,)\n",
      "(3147,)\n",
      "(3147,)\n",
      "(3147,)\n"
     ]
    }
   ],
   "source": [
    "#we now make the predictions\n",
    "predictions_arr = []\n",
    "for i in range(5):\n",
    "    rightSplitmean = np.mean(trees[i][0][3], axis = 0)\n",
    "    pred_loop = []\n",
    "    for sample in Y_test_arr[i]:\n",
    "        pred_loop.append(predictArbitrary(sample,Di_mean[i],rightSplitmean,trees[i][2], trees[i][3], trees[i][4]))\n",
    "    predictions_arr.append(np.array(pred_loop))\n",
    "    print(predictions_arr[i].shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3147,)\n",
      "0.8068001271051796\n"
     ]
    }
   ],
   "source": [
    "#we now conduct the majority vote by taking the mode of the predictions across the 5 trees\n",
    "final_predictions = []\n",
    "for i in range(len(predictions_arr[0])):\n",
    "    pred_count = {0:0,1:0,2:0}\n",
    "    for j in range(5):\n",
    "        pred_count[predictions_arr[j][i]] += 1\n",
    "    final_predictions.append(max(pred_count, key = pred_count.get))\n",
    "    \n",
    "final_predictions = np.array(final_predictions)\n",
    "print(final_predictions.shape)\n",
    "\n",
    "#we calculate the accuracy\n",
    "accuracy = np.sum(final_predictions == testYlabels)/len(final_predictions)\n",
    "print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9908163265306122\n",
      "0.9039647577092511\n",
      "0.5251937984496124\n"
     ]
    }
   ],
   "source": [
    "#class wise accuracy\n",
    "class0_accuracy = np.sum(final_predictions[testYlabels == 0] == 0)/np.sum(testYlabels == 0)\n",
    "class1_accuracy = np.sum(final_predictions[testYlabels == 1] == 1)/np.sum(testYlabels == 1)\n",
    "class2_accuracy = np.sum(final_predictions[testYlabels == 2] == 2)/np.sum(testYlabels == 2)\n",
    "print(class0_accuracy)\n",
    "print(class1_accuracy)\n",
    "print(class2_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
